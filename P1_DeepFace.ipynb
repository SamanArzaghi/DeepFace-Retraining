{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhOgY3oWhiPn"
      },
      "source": [
        "# Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE0-8tvmCfWZ"
      },
      "source": [
        "At first, we will unzip deepface source code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taXzvfJ3QDmC",
        "outputId": "66df0a91-a9f8-43e2-f0ea-6182410b08ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  deepface.zip\n",
            "replace deepface/__init__.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            " extracting: deepface/__init__.py    \n",
            "replace deepface/basemodels/__init__.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            " extracting: deepface/basemodels/__init__.py  \n",
            "  inflating: deepface/basemodels/ArcFace.py  \n",
            "  inflating: deepface/basemodels/Boosting.py  \n",
            "  inflating: deepface/basemodels/DeepID.py  \n",
            "  inflating: deepface/basemodels/DlibResNet.py  \n",
            "  inflating: deepface/basemodels/DlibWrapper.py  \n",
            "  inflating: deepface/basemodels/Facenet.py  \n",
            "  inflating: deepface/basemodels/Facenet512.py  \n",
            "  inflating: deepface/basemodels/FbDeepFace.py  \n",
            "  inflating: deepface/basemodels/OpenFace.py  \n",
            "  inflating: deepface/basemodels/VGGFace.py  \n",
            " extracting: deepface/commons/__init__.py  \n",
            "  inflating: deepface/commons/distance.py  \n",
            "  inflating: deepface/commons/functions.py  \n",
            "  inflating: deepface/commons/realtime.py  \n",
            "  inflating: deepface/DeepFace.py    \n",
            " extracting: deepface/detectors/__init__.py  \n",
            "  inflating: deepface/detectors/DlibWrapper.py  \n",
            "  inflating: deepface/detectors/FaceDetector.py  \n",
            "  inflating: deepface/detectors/MtcnnWrapper.py  \n",
            "  inflating: deepface/detectors/OpenCvWrapper.py  \n",
            "  inflating: deepface/detectors/RetinaFaceWrapper.py  \n",
            "  inflating: deepface/detectors/SsdWrapper.py  \n",
            " extracting: deepface/extendedmodels/__init__.py  \n",
            "  inflating: deepface/extendedmodels/Age.py  \n",
            "  inflating: deepface/extendedmodels/Emotion.py  \n",
            "  inflating: deepface/extendedmodels/Gender.py  \n",
            "  inflating: deepface/extendedmodels/Race.py  \n",
            " extracting: deepface/models/__init__.py  \n",
            "  inflating: deepface/models/face-recognition-ensemble-model.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip deepface.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSQVUF0Hht84"
      },
      "source": [
        "Now we need to use analysis fundtion from deepface to do the first task..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s8vY4RFTUhY",
        "outputId": "64dad972-caa7-4df3-985a-06cfc8f304cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory  /root /.deepface created\n",
            "Directory  /root /.deepface/weights created\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import time\n",
        "import re\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "from deepface import DeepFace\n",
        "from deepface.extendedmodels import Age\n",
        "from deepface.commons import functions, realtime, distance as dst\n",
        "from deepface.detectors import FaceDetector\n",
        "\n",
        "def analysis(db_path, model_name = 'VGG-Face', detector_backend = 'opencv', distance_metric = 'cosine', enable_face_analysis = True, source = '/content/saman_clip.mp4', time_threshold = 5, frame_threshold = 5):\n",
        "\n",
        "\t#------------------------\n",
        "\n",
        "\tface_detector = FaceDetector.build_model(detector_backend)\n",
        "\tprint(\"Detector backend is \", detector_backend)\n",
        "\n",
        "\t#------------------------\n",
        "\n",
        "\tinput_shape = (224, 224); input_shape_x = input_shape[0]; input_shape_y = input_shape[1]\n",
        "\n",
        "\ttext_color = (255,255,255)\n",
        "\n",
        "\temployees = []\n",
        "\t#check passed db folder exists\n",
        "\tif os.path.isdir(db_path) == True:\n",
        "\t\tfor r, d, f in os.walk(db_path): # r=root, d=directories, f = files\n",
        "\t\t\tfor file in f:\n",
        "\t\t\t\tif ('.jpg' in file):\n",
        "\t\t\t\t\t#exact_path = os.path.join(r, file)\n",
        "\t\t\t\t\texact_path = r + \"/\" + file\n",
        "\t\t\t\t\t#print(exact_path)\n",
        "\t\t\t\t\temployees.append(exact_path)\n",
        "\n",
        "\tif len(employees) == 0:\n",
        "\t\tprint(\"WARNING: There is no image in this path ( \", db_path,\") . Face recognition will not be performed.\")\n",
        "\n",
        "\t#------------------------\n",
        "\n",
        "\tif len(employees) > 0:\n",
        "\n",
        "\t\tmodel = DeepFace.build_model(model_name)\n",
        "\t\tprint(model_name,\" is built\")\n",
        "\n",
        "\t\t#------------------------\n",
        "\n",
        "\t\tinput_shape = functions.find_input_shape(model)\n",
        "\t\tinput_shape_x = input_shape[0]; input_shape_y = input_shape[1]\n",
        "\n",
        "\t\t#tuned thresholds for model and metric pair\n",
        "\t\tthreshold = dst.findThreshold(model_name, distance_metric)\n",
        "\n",
        "\t#------------------------\n",
        "\t#facial attribute analysis models\n",
        "\n",
        "\tif enable_face_analysis == True:\n",
        "\n",
        "\t\ttic = time.time()\n",
        "\n",
        "\t\temotion_model = DeepFace.build_model('Emotion')\n",
        "\t\tprint(\"Emotion model loaded\")\n",
        "\n",
        "\t\tage_model = DeepFace.build_model('Age')\n",
        "\t\tprint(\"Age model loaded\")\n",
        "\n",
        "\t\tgender_model = DeepFace.build_model('Gender')\n",
        "\t\tprint(\"Gender model loaded\")\n",
        "\n",
        "\t\ttoc = time.time()\n",
        "\n",
        "\t\tprint(\"Facial attibute analysis models loaded in \",toc-tic,\" seconds\")\n",
        "\n",
        "\t#------------------------\n",
        "\n",
        "\t#find embeddings for employee list\n",
        "\n",
        "\ttic = time.time()\n",
        "\n",
        "\t#-----------------------\n",
        "\n",
        "\tpbar = tqdm(range(0, len(employees)), desc='Finding embeddings')\n",
        "\n",
        "\t#TODO: why don't you store those embeddings in a pickle file similar to find function?\n",
        "\n",
        "\tembeddings = []\n",
        "\t#for employee in employees:\n",
        "\tfor index in pbar:\n",
        "\t\temployee = employees[index]\n",
        "\t\tpbar.set_description(\"Finding embedding for %s\" % (employee.split(\"/\")[-1]))\n",
        "\t\tembedding = []\n",
        "\n",
        "\t\t#preprocess_face returns single face. this is expected for source images in db.\n",
        "\t\timg = functions.preprocess_face(img = employee, target_size = (input_shape_y, input_shape_x), enforce_detection = False, detector_backend = detector_backend)\n",
        "\t\timg_representation = model.predict(img)[0,:]\n",
        "\n",
        "\t\tembedding.append(employee)\n",
        "\t\tembedding.append(img_representation)\n",
        "\t\tembeddings.append(embedding)\n",
        "\n",
        "\tdf = pd.DataFrame(embeddings, columns = ['employee', 'embedding'])\n",
        "\tdf['distance_metric'] = distance_metric\n",
        "\n",
        "\ttoc = time.time()\n",
        "\n",
        "\tprint(\"Embeddings found for given data set in \", toc-tic,\" seconds\")\n",
        "\n",
        "\t#-----------------------\n",
        "\n",
        "\tpivot_img_size = 112 #face recognition result image\n",
        "\n",
        "\t#-----------------------\n",
        "\n",
        "\tfreeze = False\n",
        "\tface_detected = False\n",
        "\tface_included_frames = 0 #freeze screen if face detected sequantially 5 frames\n",
        "\tfreezed_frame = 0\n",
        "\ttic = time.time()\n",
        "\n",
        "\tcap = cv2.VideoCapture(source) #webcam\n",
        "\n",
        "\twhile(True):\n",
        "\t\tret, img = cap.read()\n",
        "\n",
        "\t\tif img is None:\n",
        "\t\t\tbreak\n",
        "\n",
        "\t\t#cv2.namedWindow('img', cv2.WINDOW_FREERATIO)\n",
        "\t\t#cv2.setWindowProperty('img', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
        "\n",
        "\t\traw_img = img.copy()\n",
        "\t\tresolution = img.shape; resolution_x = img.shape[1]; resolution_y = img.shape[0]\n",
        "\n",
        "\t\tif freeze == False:\n",
        "\n",
        "\t\t\ttry:\n",
        "\t\t\t\t#faces store list of detected_face and region pair\n",
        "\t\t\t\tfaces = FaceDetector.detect_faces(face_detector, detector_backend, img, align = False)\n",
        "\t\t\texcept: #to avoid exception if no face detected\n",
        "\t\t\t\tfaces = []\n",
        "\n",
        "\t\t\tif len(faces) == 0:\n",
        "\t\t\t\tface_included_frames = 0\n",
        "\t\telse:\n",
        "\t\t\tfaces = []\n",
        "\n",
        "\t\tdetected_faces = []\n",
        "\t\tface_index = 0\n",
        "\t\tfor face, (x, y, w, h) in faces:\n",
        "\t\t\tif w > 130: #discard small detected faces\n",
        "\n",
        "\t\t\t\tface_detected = True\n",
        "\t\t\t\tif face_index == 0:\n",
        "\t\t\t\t\tface_included_frames = face_included_frames + 1 #increase frame for a single face\n",
        "\n",
        "\t\t\t\tcv2.rectangle(img, (x,y), (x+w,y+h), (67,67,67), 1) #draw rectangle to main image\n",
        "\n",
        "\t\t\t\tcv2.putText(img, str(frame_threshold - face_included_frames), (int(x+w/4),int(y+h/1.5)), cv2.FONT_HERSHEY_SIMPLEX, 4, (255, 255, 255), 2)\n",
        "\n",
        "\t\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n",
        "\n",
        "\t\t\t\t#-------------------------------------\n",
        "\n",
        "\t\t\t\tdetected_faces.append((x,y,w,h))\n",
        "\t\t\t\tface_index = face_index + 1\n",
        "\n",
        "\t\t\t\t#-------------------------------------\n",
        "\n",
        "\t\tif face_detected == True and face_included_frames == frame_threshold and freeze == False:\n",
        "\t\t\tfreeze = True\n",
        "\t\t\t#base_img = img.copy()\n",
        "\t\t\tbase_img = raw_img.copy()\n",
        "\t\t\tdetected_faces_final = detected_faces.copy()\n",
        "\t\t\ttic = time.time()\n",
        "\n",
        "\t\tif freeze == True:\n",
        "\n",
        "\t\t\ttoc = time.time()\n",
        "\t\t\tif (toc - tic) < time_threshold:\n",
        "\n",
        "\t\t\t\tif freezed_frame == 0:\n",
        "\t\t\t\t\tfreeze_img = base_img.copy()\n",
        "\t\t\t\t\t#freeze_img = np.zeros(resolution, np.uint8) #here, np.uint8 handles showing white area issue\n",
        "\n",
        "\t\t\t\t\tfor detected_face in detected_faces_final:\n",
        "\t\t\t\t\t\tx = detected_face[0]; y = detected_face[1]\n",
        "\t\t\t\t\t\tw = detected_face[2]; h = detected_face[3]\n",
        "\n",
        "\t\t\t\t\t\tcv2.rectangle(freeze_img, (x,y), (x+w,y+h), (67,67,67), 1) #draw rectangle to main image\n",
        "\n",
        "\t\t\t\t\t\t#-------------------------------\n",
        "\n",
        "\t\t\t\t\t\t#apply deep learning for custom_face\n",
        "\n",
        "\t\t\t\t\t\tcustom_face = base_img[y:y+h, x:x+w]\n",
        "\n",
        "\t\t\t\t\t\t#-------------------------------\n",
        "\t\t\t\t\t\t#facial attribute analysis\n",
        "\n",
        "\t\t\t\t\t\tif enable_face_analysis == True:\n",
        "\n",
        "\t\t\t\t\t\t\tgray_img = functions.preprocess_face(img = custom_face, target_size = (48, 48), grayscale = True, enforce_detection = False, detector_backend = 'opencv')\n",
        "\t\t\t\t\t\t\temotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\t\t\t\t\t\t\temotion_predictions = emotion_model.predict(gray_img)[0,:]\n",
        "\t\t\t\t\t\t\tsum_of_predictions = emotion_predictions.sum()\n",
        "\n",
        "\t\t\t\t\t\t\tmood_items = []\n",
        "\t\t\t\t\t\t\tfor i in range(0, len(emotion_labels)):\n",
        "\t\t\t\t\t\t\t\tmood_item = []\n",
        "\t\t\t\t\t\t\t\temotion_label = emotion_labels[i]\n",
        "\t\t\t\t\t\t\t\temotion_prediction = 100 * emotion_predictions[i] / sum_of_predictions\n",
        "\t\t\t\t\t\t\t\tmood_item.append(emotion_label)\n",
        "\t\t\t\t\t\t\t\tmood_item.append(emotion_prediction)\n",
        "\t\t\t\t\t\t\t\tmood_items.append(mood_item)\n",
        "\n",
        "\t\t\t\t\t\t\temotion_df = pd.DataFrame(mood_items, columns = [\"emotion\", \"score\"])\n",
        "\t\t\t\t\t\t\temotion_df = emotion_df.sort_values(by = [\"score\"], ascending=False).reset_index(drop=True)\n",
        "\n",
        "\t\t\t\t\t\t\t#background of mood box\n",
        "\n",
        "\t\t\t\t\t\t\t#transparency\n",
        "\t\t\t\t\t\t\toverlay = freeze_img.copy()\n",
        "\t\t\t\t\t\t\topacity = 0.4\n",
        "\n",
        "\t\t\t\t\t\t\tif x+w+pivot_img_size < resolution_x:\n",
        "\t\t\t\t\t\t\t\t#right\n",
        "\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img\n",
        "\t\t\t\t\t\t\t\t\t#, (x+w,y+20)\n",
        "\t\t\t\t\t\t\t\t\t, (x+w,y)\n",
        "\t\t\t\t\t\t\t\t\t, (x+w+pivot_img_size, y+h)\n",
        "\t\t\t\t\t\t\t\t\t, (64,64,64),cv2.FILLED)\n",
        "\n",
        "\t\t\t\t\t\t\t\tcv2.addWeighted(overlay, opacity, freeze_img, 1 - opacity, 0, freeze_img)\n",
        "\n",
        "\t\t\t\t\t\t\telif x-pivot_img_size > 0:\n",
        "\t\t\t\t\t\t\t\t#left\n",
        "\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img\n",
        "\t\t\t\t\t\t\t\t\t#, (x-pivot_img_size,y+20)\n",
        "\t\t\t\t\t\t\t\t\t, (x-pivot_img_size,y)\n",
        "\t\t\t\t\t\t\t\t\t, (x, y+h)\n",
        "\t\t\t\t\t\t\t\t\t, (64,64,64),cv2.FILLED)\n",
        "\n",
        "\t\t\t\t\t\t\t\tcv2.addWeighted(overlay, opacity, freeze_img, 1 - opacity, 0, freeze_img)\n",
        "\n",
        "\t\t\t\t\t\t\tfor index, instance in emotion_df.iterrows():\n",
        "\t\t\t\t\t\t\t\temotion_label = \"%s \" % (instance['emotion'])\n",
        "\t\t\t\t\t\t\t\temotion_score = instance['score']/100\n",
        "\n",
        "\t\t\t\t\t\t\t\tbar_x = 35 #this is the size if an emotion is 100%\n",
        "\t\t\t\t\t\t\t\tbar_x = int(bar_x * emotion_score)\n",
        "\n",
        "\t\t\t\t\t\t\t\tif x+w+pivot_img_size < resolution_x:\n",
        "\n",
        "\t\t\t\t\t\t\t\t\ttext_location_y = y + 20 + (index+1) * 20\n",
        "\t\t\t\t\t\t\t\t\ttext_location_x = x+w\n",
        "\n",
        "\t\t\t\t\t\t\t\t\tif text_location_y < y + h:\n",
        "\t\t\t\t\t\t\t\t\t\tcv2.putText(freeze_img, emotion_label, (text_location_x, text_location_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img\n",
        "\t\t\t\t\t\t\t\t\t\t\t, (x+w+70, y + 13 + (index+1) * 20)\n",
        "\t\t\t\t\t\t\t\t\t\t\t, (x+w+70+bar_x, y + 13 + (index+1) * 20 + 5)\n",
        "\t\t\t\t\t\t\t\t\t\t\t, (255,255,255), cv2.FILLED)\n",
        "\n",
        "\t\t\t\t\t\t\t\telif x-pivot_img_size > 0:\n",
        "\n",
        "\t\t\t\t\t\t\t\t\ttext_location_y = y + 20 + (index+1) * 20\n",
        "\t\t\t\t\t\t\t\t\ttext_location_x = x-pivot_img_size\n",
        "\n",
        "\t\t\t\t\t\t\t\t\tif text_location_y <= y+h:\n",
        "\t\t\t\t\t\t\t\t\t\tcv2.putText(freeze_img, emotion_label, (text_location_x, text_location_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img\n",
        "\t\t\t\t\t\t\t\t\t\t\t, (x-pivot_img_size+70, y + 13 + (index+1) * 20)\n",
        "\t\t\t\t\t\t\t\t\t\t\t, (x-pivot_img_size+70+bar_x, y + 13 + (index+1) * 20 + 5)\n",
        "\t\t\t\t\t\t\t\t\t\t\t, (255,255,255), cv2.FILLED)\n",
        "\n",
        "\t\t\t\t\t\t\t#-------------------------------\n",
        "\n",
        "\t\t\t\t\t\t\tface_224 = functions.preprocess_face(img = custom_face, target_size = (224, 224), grayscale = False, enforce_detection = False, detector_backend = 'opencv')\n",
        "\n",
        "\t\t\t\t\t\t\tage_predictions = age_model.predict(face_224)[0,:]\n",
        "\t\t\t\t\t\t\tapparent_age = Age.findApparentAge(age_predictions)\n",
        "\n",
        "\t\t\t\t\t\t\t#-------------------------------\n",
        "\n",
        "\t\t\t\t\t\t\tgender_prediction = gender_model.predict(face_224)[0,:]\n",
        "\n",
        "\t\t\t\t\t\t\tif np.argmax(gender_prediction) == 0:\n",
        "\t\t\t\t\t\t\t\tgender = \"W\"\n",
        "\t\t\t\t\t\t\telif np.argmax(gender_prediction) == 1:\n",
        "\t\t\t\t\t\t\t\tgender = \"M\"\n",
        "\n",
        "\t\t\t\t\t\t\t#print(str(int(apparent_age)),\" years old \", dominant_emotion, \" \", gender)\n",
        "\n",
        "\t\t\t\t\t\t\tanalysis_report = str(int(apparent_age))+\" \"+gender\n",
        "\n",
        "\t\t\t\t\t\t\t#-------------------------------\n",
        "\n",
        "\t\t\t\t\t\t\tinfo_box_color = (46,200,255)\n",
        "\n",
        "\t\t\t\t\t\t\t#top\n",
        "\t\t\t\t\t\t\tif y - pivot_img_size + int(pivot_img_size/5) > 0:\n",
        "\n",
        "\t\t\t\t\t\t\t\ttriangle_coordinates = np.array( [\n",
        "\t\t\t\t\t\t\t\t\t(x+int(w/2), y)\n",
        "\t\t\t\t\t\t\t\t\t, (x+int(w/2)-int(w/10), y-int(pivot_img_size/3))\n",
        "\t\t\t\t\t\t\t\t\t, (x+int(w/2)+int(w/10), y-int(pivot_img_size/3))\n",
        "\t\t\t\t\t\t\t\t] )\n",
        "\n",
        "\t\t\t\t\t\t\t\tcv2.drawContours(freeze_img, [triangle_coordinates], 0, info_box_color, -1)\n",
        "\n",
        "\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img, (x+int(w/5), y-pivot_img_size+int(pivot_img_size/5)), (x+w-int(w/5), y-int(pivot_img_size/3)), info_box_color, cv2.FILLED)\n",
        "\n",
        "\t\t\t\t\t\t\t\tcv2.putText(freeze_img, analysis_report, (x+int(w/3.5), y - int(pivot_img_size/2.1)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 111, 255), 2)\n",
        "\n",
        "\t\t\t\t\t\t\t#bottom\n",
        "\t\t\t\t\t\t\telif y + h + pivot_img_size - int(pivot_img_size/5) < resolution_y:\n",
        "\n",
        "\t\t\t\t\t\t\t\ttriangle_coordinates = np.array( [\n",
        "\t\t\t\t\t\t\t\t\t(x+int(w/2), y+h)\n",
        "\t\t\t\t\t\t\t\t\t, (x+int(w/2)-int(w/10), y+h+int(pivot_img_size/3))\n",
        "\t\t\t\t\t\t\t\t\t, (x+int(w/2)+int(w/10), y+h+int(pivot_img_size/3))\n",
        "\t\t\t\t\t\t\t\t] )\n",
        "\n",
        "\t\t\t\t\t\t\t\tcv2.drawContours(freeze_img, [triangle_coordinates], 0, info_box_color, -1)\n",
        "\n",
        "\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img, (x+int(w/5), y + h + int(pivot_img_size/3)), (x+w-int(w/5), y+h+pivot_img_size-int(pivot_img_size/5)), info_box_color, cv2.FILLED)\n",
        "\n",
        "\t\t\t\t\t\t\t\tcv2.putText(freeze_img, analysis_report, (x+int(w/3.5), y + h + int(pivot_img_size/1.5)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 111, 255), 2)\n",
        "\n",
        "\t\t\t\t\t\t#-------------------------------\n",
        "\t\t\t\t\t\t#face recognition\n",
        "\n",
        "\t\t\t\t\t\tcustom_face = functions.preprocess_face(img = custom_face, target_size = (input_shape_y, input_shape_x), enforce_detection = False, detector_backend = 'opencv')\n",
        "\n",
        "\t\t\t\t\t\t#check preprocess_face function handled\n",
        "\t\t\t\t\t\tif custom_face.shape[1:3] == input_shape:\n",
        "\t\t\t\t\t\t\tif df.shape[0] > 0: #if there are images to verify, apply face recognition\n",
        "\t\t\t\t\t\t\t\timg1_representation = model.predict(custom_face)[0,:]\n",
        "\n",
        "\t\t\t\t\t\t\t\t#print(freezed_frame,\" - \",img1_representation[0:5])\n",
        "\n",
        "\t\t\t\t\t\t\t\tdef findDistance(row):\n",
        "\t\t\t\t\t\t\t\t\tdistance_metric = row['distance_metric']\n",
        "\t\t\t\t\t\t\t\t\timg2_representation = row['embedding']\n",
        "\n",
        "\t\t\t\t\t\t\t\t\tdistance = 1000 #initialize very large value\n",
        "\t\t\t\t\t\t\t\t\tif distance_metric == 'cosine':\n",
        "\t\t\t\t\t\t\t\t\t\tdistance = dst.findCosineDistance(img1_representation, img2_representation)\n",
        "\t\t\t\t\t\t\t\t\telif distance_metric == 'euclidean':\n",
        "\t\t\t\t\t\t\t\t\t\tdistance = dst.findEuclideanDistance(img1_representation, img2_representation)\n",
        "\t\t\t\t\t\t\t\t\telif distance_metric == 'euclidean_l2':\n",
        "\t\t\t\t\t\t\t\t\t\tdistance = dst.findEuclideanDistance(dst.l2_normalize(img1_representation), dst.l2_normalize(img2_representation))\n",
        "\n",
        "\t\t\t\t\t\t\t\t\treturn distance\n",
        "\n",
        "\t\t\t\t\t\t\t\tdf['distance'] = df.apply(findDistance, axis = 1)\n",
        "\t\t\t\t\t\t\t\tdf = df.sort_values(by = [\"distance\"])\n",
        "\n",
        "\t\t\t\t\t\t\t\tcandidate = df.iloc[0]\n",
        "\t\t\t\t\t\t\t\temployee_name = candidate['employee']\n",
        "\t\t\t\t\t\t\t\tbest_distance = candidate['distance']\n",
        "\n",
        "\t\t\t\t\t\t\t\t#print(candidate[['employee', 'distance']].values)\n",
        "\n",
        "\t\t\t\t\t\t\t\t#if True:\n",
        "\t\t\t\t\t\t\t\tif best_distance <= threshold:\n",
        "\t\t\t\t\t\t\t\t\t#print(employee_name)\n",
        "\t\t\t\t\t\t\t\t\tdisplay_img = cv2.imread(employee_name)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\tdisplay_img = cv2.resize(display_img, (pivot_img_size, pivot_img_size))\n",
        "\n",
        "\t\t\t\t\t\t\t\t\tlabel = employee_name.split(\"/\")[-1].replace(\".jpg\", \"\")\n",
        "\t\t\t\t\t\t\t\t\tlabel = re.sub('[0-9]', '', label)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\t\t\t\t\tif y - pivot_img_size > 0 and x + w + pivot_img_size < resolution_x:\n",
        "\t\t\t\t\t\t\t\t\t\t\t#top right\n",
        "\t\t\t\t\t\t\t\t\t\t\tfreeze_img[y - pivot_img_size:y, x+w:x+w+pivot_img_size] = display_img\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t\toverlay = freeze_img.copy(); opacity = 0.4\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img,(x+w,y),(x+w+pivot_img_size, y+20),(46,200,255),cv2.FILLED)\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.addWeighted(overlay, opacity, freeze_img, 1 - opacity, 0, freeze_img)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.putText(freeze_img, label, (x+w, y+10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t\t#connect face and text\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.line(freeze_img,(x+int(w/2), y), (x+3*int(w/4), y-int(pivot_img_size/2)),(67,67,67),1)\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.line(freeze_img, (x+3*int(w/4), y-int(pivot_img_size/2)), (x+w, y - int(pivot_img_size/2)), (67,67,67),1)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\telif y + h + pivot_img_size < resolution_y and x - pivot_img_size > 0:\n",
        "\t\t\t\t\t\t\t\t\t\t\t#bottom left\n",
        "\t\t\t\t\t\t\t\t\t\t\tfreeze_img[y+h:y+h+pivot_img_size, x-pivot_img_size:x] = display_img\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t\toverlay = freeze_img.copy(); opacity = 0.4\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img,(x-pivot_img_size,y+h-20),(x, y+h),(46,200,255),cv2.FILLED)\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.addWeighted(overlay, opacity, freeze_img, 1 - opacity, 0, freeze_img)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.putText(freeze_img, label, (x - pivot_img_size, y+h-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t\t#connect face and text\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.line(freeze_img,(x+int(w/2), y+h), (x+int(w/2)-int(w/4), y+h+int(pivot_img_size/2)),(67,67,67),1)\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.line(freeze_img, (x+int(w/2)-int(w/4), y+h+int(pivot_img_size/2)), (x, y+h+int(pivot_img_size/2)), (67,67,67),1)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\telif y - pivot_img_size > 0 and x - pivot_img_size > 0:\n",
        "\t\t\t\t\t\t\t\t\t\t\t#top left\n",
        "\t\t\t\t\t\t\t\t\t\t\tfreeze_img[y-pivot_img_size:y, x-pivot_img_size:x] = display_img\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t\toverlay = freeze_img.copy(); opacity = 0.4\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img,(x- pivot_img_size,y),(x, y+20),(46,200,255),cv2.FILLED)\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.addWeighted(overlay, opacity, freeze_img, 1 - opacity, 0, freeze_img)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.putText(freeze_img, label, (x - pivot_img_size, y+10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t\t#connect face and text\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.line(freeze_img,(x+int(w/2), y), (x+int(w/2)-int(w/4), y-int(pivot_img_size/2)),(67,67,67),1)\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.line(freeze_img, (x+int(w/2)-int(w/4), y-int(pivot_img_size/2)), (x, y - int(pivot_img_size/2)), (67,67,67),1)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\telif x+w+pivot_img_size < resolution_x and y + h + pivot_img_size < resolution_y:\n",
        "\t\t\t\t\t\t\t\t\t\t\t#bottom righ\n",
        "\t\t\t\t\t\t\t\t\t\t\tfreeze_img[y+h:y+h+pivot_img_size, x+w:x+w+pivot_img_size] = display_img\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t\toverlay = freeze_img.copy(); opacity = 0.4\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.rectangle(freeze_img,(x+w,y+h-20),(x+w+pivot_img_size, y+h),(46,200,255),cv2.FILLED)\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.addWeighted(overlay, opacity, freeze_img, 1 - opacity, 0, freeze_img)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.putText(freeze_img, label, (x+w, y+h-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t\t#connect face and text\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.line(freeze_img,(x+int(w/2), y+h), (x+int(w/2)+int(w/4), y+h+int(pivot_img_size/2)),(67,67,67),1)\n",
        "\t\t\t\t\t\t\t\t\t\t\tcv2.line(freeze_img, (x+int(w/2)+int(w/4), y+h+int(pivot_img_size/2)), (x+w, y+h+int(pivot_img_size/2)), (67,67,67),1)\n",
        "\t\t\t\t\t\t\t\t\texcept Exception as err:\n",
        "\t\t\t\t\t\t\t\t\t\tprint(str(err))\n",
        "\n",
        "\t\t\t\t\t\ttic = time.time() #in this way, freezed image can show 5 seconds\n",
        "\n",
        "\t\t\t\t\t\t#-------------------------------\n",
        "\n",
        "\t\t\t\ttime_left = int(time_threshold - (toc - tic) + 1)\n",
        "\n",
        "\t\t\t\tcv2.rectangle(freeze_img, (10, 10), (90, 50), (67,67,67), -10)\n",
        "\t\t\t\tcv2.putText(freeze_img, str(time_left), (40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 1)\n",
        "\n",
        "\t\t\t\tcv2_imshow(freeze_img)\n",
        "\n",
        "\t\t\t\tfreezed_frame = freezed_frame + 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tface_detected = False\n",
        "\t\t\t\tface_included_frames = 0\n",
        "\t\t\t\tfreeze = False\n",
        "\t\t\t\tfreezed_frame = 0\n",
        "\n",
        "\t\telse:\n",
        "\t\t\tcv2_imshow(img)\n",
        "\n",
        "\t\tif cv2.waitKey(1) & 0xFF == ord('q'): #press q to quit\n",
        "\t\t\tbreak\n",
        "\n",
        "\t#kill open cv things\n",
        "\tcap.release()\n",
        "\tcv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rmhIoBeCpAG"
      },
      "source": [
        "Let's test our function with a short video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW8ZxJY9CJs9"
      },
      "outputs": [],
      "source": [
        "analysis(db_path='/content/database', model_name = 'VGG-Face', detector_backend = 'opencv', distance_metric = 'cosine', enable_face_analysis = True, source = '/content/saman_clip.mp4', time_threshold = 5, frame_threshold = 1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}